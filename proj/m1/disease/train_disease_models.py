
# -*- coding: utf-8 -*-
"""
train_disease_models.py
ÙŠØ¯Ø±Ø¨ Ù…ØµÙ†ÙØ§Øª Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ù„ÙƒÙ„ Ù‚Ø³Ù… Ø¨Ø´ÙƒÙ„ Ù‡Ø±Ù…ÙŠ (Department -> Disease) Ù…Ø¹ Ù…Ø²Ø¬ ÙˆØ²Ù†ÙŠ Ù„Ù„Ø£Ø­ØªÙ…Ø§Ù„Ø§Øª
ÙˆÙŠØ­ÙØ¸ ÙƒÙ„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… ÙÙŠ Ø­Ø²Ù…Ø© ÙˆØ§Ø­Ø¯Ø© (disease_models.pkl).

Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©:
- CSV ÙŠØ­ÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù‚Ù„ Ø¹Ù…ÙˆØ¯ÙŠÙ†: Department (Ø§Ù„Ù‚Ø³Ù…) Ùˆ Disease (Ø§Ù„Ù…Ø±Ø¶)ØŒ ÙˆØ¨Ø§Ù‚ÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù‡ÙŠ Ù…ÙŠØ²Ø§Øª/Ø£Ø¹Ø±Ø§Ø¶ Ø±Ù‚Ù…ÙŠØ© (0/1).
- Ù†ÙØ³ Ù…Ù†Ø·Ù‚ Ø§Ø´ØªÙ‚Ø§Ù‚ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù‚Ø³Ù… (count_positive, pct_positive, Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª).

Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª:
- disease_models.pkl : Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ÙˆÙŠ:
    {
      'feature_cols': [...],
      'group_defs': {...},
      'derived_cols': [...],
      'departments': {
          dept_name: {
              'label_encoder': LabelEncoder Ù„Ù„Ø£Ù…Ø±Ø§Ø¶,
              'feature_cols': [...],
              'rf_cal', 'gb_cal', 'et_cal', 'stack' (Ù‚Ø¯ ØªÙƒÙˆÙ† None),
              'best_weights': {'rf':..,'gb':..,'et':.., ÙˆØ±Ø¨Ù…Ø§ Ø£Ø³Ù…Ø§Ø¡ Ø¥Ø¶Ø§ÙÙŠØ©},
              'extra_estimators': [(name, estimator), ...],
              'classes_': [Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨]
          },
          ...
      }
    }

Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… (Ø£Ù…Ø«Ù„Ø©):
python train_disease_models.py --csv data.csv --dept_col Department --disease_col Disease --out disease_models.pkl
python train_disease_models.py --csv data.csv --dept_col Department --disease_col Disease --min_per_class 8 --n_splits 5 --seed 42
"""

import os
import argparse
import pickle
import numpy as np
import pandas as pd
from collections import defaultdict
from itertools import product

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import f1_score
from sklearn.calibration import CalibratedClassifierCV
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier

# Ø§Ù„Ø±Ø³Ù… ØºÙŠØ± Ù…Ø·Ù„ÙˆØ¨ Ù‡Ù†Ø§ØŒ ÙÙ‚Ø· ØªØ¯Ø±ÙŠØ¨ ÙˆØ­ÙØ¸

# ===== ØªØ¹Ø±ÙŠÙØ§Øª Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø£Ø¹Ø±Ø§Ø¶ (ÙƒÙ…Ø§ ÙÙŠ Ø³ÙƒØ±Ø¨Øª Ø§Ù„Ù‚Ø³Ù…) =====
GROUPS = {
    "respiratory_idx": ['cough','wheez','breath','sputum','chest','dyspnea','asthma','pneumonia','bronch','apnea','congestion'],
    "cardiac_idx"    : ['heart','cardio','coronary','myocard','pulse','blood pressure','hypertens','tachy','cyanosis'],
    "neurology_idx"  : ['seiz','paralys','tremor','numb','weak','headache','migraine','vision','optic','speech','walking','confusion','dizziness','balance','myelitis','neuritis'],
    "ent_idx"        : ['ear','throat','nose','sinus','tonsil','hoarseness','post-nasal'],
    "derm_idx"       : ['rash','itch','skin','psoriasis','eczema','acne','scaly','thickening','redness','lesion','pigment'],
    "gi_idx"         : ['abdominal','diarr','vomit','liver','hepat','pancrea','bowel','stool','colitis','crohn','append','spleen','ascites','jaundice','hepatitis'],
    "uro_idx"        : ['urine','urinary','bladder','kidney','renal','testic','prostat','pelvic','neph'],
    "heme_idx"       : ['bleed','anemia','bruis','clot','lymph','spleen','platelet'],
    "endocrine_idx"  : ['thyroid','hypo','hyper','diabet','glycem','hormone'],
    "immune_idx"     : ['auto','lupus','raynaud'],
}
DERIVED_COLS = ["count_positive", "pct_positive"] + list(GROUPS.keys())

def add_group_index(X: pd.DataFrame, features, name: str, keywords):
    cols = np.array(features)
    mask = np.zeros(len(cols), dtype=bool)
    for kw in keywords:
        mask |= np.array([kw.lower() in c.lower() for c in cols])
    X[name] = X.loc[:, cols[mask]].sum(axis=1) if mask.any() else 0.0
    return X

def build_features_like_training(df: pd.DataFrame, feature_cols_training=None,
                                 target_cols=None, ignore_cols=None):
    """
    ÙŠØ¹ÙŠØ¯ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙŠØ²Ø§Øª (Ø§Ù„Ø£ØµÙ„ÙŠØ© + Ø§Ù„Ù…Ø´ØªÙ‚Ø©) Ø¨Ù†ÙØ³ Ù…Ù†Ø·Ù‚ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙÙŠ Ø§Ù„Ù‚Ø³Ù….
    Ø¥Ø°Ø§ ÙƒØ§Ù†Øª feature_cols_training = None Ø³ÙŠØªÙ… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø«Ù… ØªØ«Ø¨ÙŠØªÙ‡Ø§.
    """
    target_cols = target_cols or []
    ignore_cols = (ignore_cols or []) + target_cols
    base_features = [c for c in df.columns if c not in ignore_cols]
    X = df[base_features].apply(pd.to_numeric, errors='coerce').fillna(0)

    # Ù…Ø´ØªÙ‚Ø§Øª Ø¹Ø§Ù…Ø©
    X["count_positive"] = X.sum(axis=1)
    X["pct_positive"] = X["count_positive"] / (len(base_features) if len(base_features) else 1)

    # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª
    for gname, kws in GROUPS.items():
        X = add_group_index(X, base_features, gname, kws)

    # ÙÙŠ Ø£ÙˆÙ„ Ù…Ø±Ø© Ù„Ø§ Ù†Ù…Ù„Ùƒ feature_cols_training: Ù†Ø«Ø¨Ù‘Øª Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¢Ù†
    if feature_cols_training is None:
        feature_cols_training = list(X.columns)

    # Ø¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯ ÙƒÙ„ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© ÙˆØ¨Ø§Ù„ØªØ±ØªÙŠØ¨
    for c in feature_cols_training:
        if c not in X.columns:
            X[c] = 0.0
    X = X.loc[:, feature_cols_training]
    return X, feature_cols_training

def proba_in_label_encoder_order(model, X: pd.DataFrame, n_classes: int, le: LabelEncoder):
    proba = model.predict_proba(X)
    proba = np.asarray(proba)
    aligned = np.zeros((proba.shape[0], n_classes), dtype=float)
    est_classes = getattr(model, "classes_", None)
    if est_classes is None:
        aligned[:, :proba.shape[1]] = proba
        return aligned
    est_classes = np.array(est_classes)
    if est_classes.dtype.kind in {"U", "S", "O"}:
        est_idx = le.transform(est_classes)
    else:
        est_idx = est_classes.astype(int)
    aligned[:, est_idx] = proba
    return aligned

def blend_proba(probas: dict, weights: dict, n_classes: int):
    """probas: dict name-> np.array(N, C), weights: name->float"""
    total_w = 0.0
    out = None
    for name, P in probas.items():
        w = float(weights.get(name, 0.0))
        if P is None or w <= 0:
            continue
        out = P if out is None else (out + w * P)
        total_w += w
    if out is None:
        return np.zeros((next(iter(probas.values())).shape[0], n_classes), dtype=float)
    return out / max(total_w, 1e-9)

def weight_grid(step=0.25, names=("rf","gb","et")):
    vals = np.arange(0, 1.0 + 1e-9, step)
    for ws in product(vals, repeat=len(names)):
        if sum(ws) == 0:
            continue
        yield {n: w for n, w in zip(names, ws)}

def make_base_estimators(seed: int):
    rf = RandomForestClassifier(
        n_estimators=400, max_depth=None, min_samples_leaf=1,
        class_weight="balanced", random_state=seed, n_jobs=-1
    )
    et = ExtraTreesClassifier(
        n_estimators=500, max_depth=None, min_samples_leaf=1,
        class_weight="balanced", random_state=seed, n_jobs=-1
    )
    gb = GradientBoostingClassifier(
        random_state=seed  # Ù„Ø§ ÙŠØ¯Ø¹Ù… class_weight Ù…Ø¨Ø§Ø´Ø±Ø©
    )
    return rf, gb, et

def choose_calibration_method(n_samples: int):
    # isotonic ÙŠØ­ØªØ§Ø¬ Ø¹ÙŠÙ†Ø§Øª Ø£ÙƒØ«Ø±Ø› sigmoid Ø£ÙƒØ«Ø± Ø§Ø³ØªÙ‚Ø±Ø§Ø±Ù‹Ø§ Ù…Ø¹ Ø§Ù„Ù‚ÙÙ„Ø©
    return "isotonic" if n_samples >= 100 else "sigmoid"

def cv_search_weights_for_dept(X, y, seed=42, n_splits=5, step=0.25, verbose=False):
    """
    ÙŠØ¨Ø­Ø« Ø¹Ù† Ø£ÙØ¶Ù„ Ø£ÙˆØ²Ø§Ù† (rf/gb/et) Ø¯Ø§Ø®Ù„ Ù‚Ø³Ù… ÙˆØ§Ø­Ø¯ ÙˆÙÙ‚ F1_macro Ø¹Ù„Ù‰ ØªØ­Ù‚Ù‚ Ù…ØªÙ‚Ø§Ø·Ø¹.
    - ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ train-fold ÙÙ‚Ø·
    - Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¹Ù„Ù‰ val-fold (Ø¨Ø¯ÙˆÙ† Ù…Ø¹Ø§ÙŠØ±Ø© Ø£Ø«Ù†Ø§Ø¡ CV Ù„ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø¨Ø­Ø«)
    - Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„ØªÙŠ ØªØ¹Ø¸Ù… Ù…ØªÙˆØ³Ø· F1_macro Ø¹Ø¨Ø± Ø§Ù„Ø·ÙŠØ§Øª
    """
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)
    model_names = ("rf","gb","et")
    weight_scores = defaultdict(list)

    for fold, (tr, va) in enumerate(skf.split(X, y), 1):
        Xtr, Xva = X.iloc[tr], X.iloc[va]
        ytr, yva = y[tr], y[va]

        rf, gb, et = make_base_estimators(seed + fold)
        rf.fit(Xtr, ytr)
        gb.fit(Xtr, ytr)
        et.fit(Xtr, ytr)

        # Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª ØºÙŠØ± Ù…ÙØ¹Ø§ÙŠØ±Ø© Ù„Ø£ØºØ±Ø§Ø¶ CV
        n_classes = len(np.unique(y))
        le_tmp = LabelEncoder().fit(y)  # Ù„Ø¶Ù…Ø§Ù† Ù…Ø­Ø§Ø°Ø§Ø© Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª
        probas = {
            "rf": proba_in_label_encoder_order(rf, Xva, n_classes, le_tmp),
            "gb": proba_in_label_encoder_order(gb, Xva, n_classes, le_tmp),
            "et": proba_in_label_encoder_order(et, Xva, n_classes, le_tmp)
        }

        for W in weight_grid(step=step, names=model_names):
            P = blend_proba(probas, W, n_classes)
            yhat = np.argmax(P, axis=1)
            f1m = f1_score(yva, yhat, average="macro")
            weight_scores[tuple(W[n] for n in model_names)].append(f1m)

    # Ù…ØªÙˆØ³Ø· Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ø¨Ø± Ø§Ù„Ø·ÙŠØ§Øª
    best_w = None
    best_mean = -1
    for ws, scores in weight_scores.items():
        mean_f1 = float(np.mean(scores)) if scores else 0.0
        if mean_f1 > best_mean:
            best_mean = mean_f1
            best_w = {name: w for name, w in zip(model_names, ws)}

    if verbose:
        print(f"[CV] Ø£ÙØ¶Ù„ Ø£ÙˆØ²Ø§Ù†: {best_w} | F1_macro_cv={best_mean:.4f}")

    return best_w or {"rf":1.0,"gb":0.0,"et":0.0}, best_mean

def train_calibrated_models_full(X, y, seed=42, calib="sigmoid"):
    """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø³Ù… + Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª."""
    rf, gb, et = make_base_estimators(seed)
    rf.fit(X, y); et.fit(X, y); gb.fit(X, y)

    # Calibrate
    rf_cal = CalibratedClassifierCV(rf, method=calib, cv=3)
    gb_cal = CalibratedClassifierCV(gb, method=calib, cv=3)
    et_cal = CalibratedClassifierCV(et, method=calib, cv=3)
    rf_cal.fit(X, y); gb_cal.fit(X, y); et_cal.fit(X, y)

    return rf_cal, gb_cal, et_cal

def main():
    ap = argparse.ArgumentParser(description="ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ø¯Ø§Ø®Ù„ ÙƒÙ„ Ù‚Ø³Ù… Ù…Ø¹ Ù…Ø²Ø¬ ÙˆØ²Ù†ÙŠ")
    ap.add_argument("--csv", type=str, required=True, help="Ù…Ù„Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    ap.add_argument("--dept_col", type=str, default="Department", help="Ø§Ø³Ù… Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù‚Ø³Ù…")
    ap.add_argument("--disease_col", type=str, default="Disease", help="Ø§Ø³Ù… Ø¹Ù…ÙˆØ¯ Ø§Ù„Ù…Ø±Ø¶")
    ap.add_argument("--ignore", nargs="*", default=[], help="Ø£Ø¹Ù…Ø¯Ø© ÙŠØªÙ… ØªØ¬Ø§Ù‡Ù„Ù‡Ø§ Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§Øª (Ø¨Ø®Ù„Ø§Ù Ø§Ù„Ù‚Ø³Ù…/Ø§Ù„Ù…Ø±Ø¶)")
    ap.add_argument("--out", type=str, default="disease_models.pkl", help="Ø§Ø³Ù… Ù…Ù„Ù Ø§Ù„Ø­Ø²Ù…Ø© Ø§Ù„Ù†Ø§ØªØ¬Ø©")
    ap.add_argument("--n_splits", type=int, default=5, help="Ø¹Ø¯Ø¯ Ø·ÙŠØ§Øª Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙ‚Ø§Ø·Ø¹ Ø¯Ø§Ø®Ù„ Ø§Ù„Ù‚Ø³Ù…")
    ap.add_argument("--min_per_class", type=int, default=5, help="Ø£Ø¯Ù†Ù‰ Ø¹Ø¯Ø¯ Ø¹ÙŠÙ†Ø§Øª Ù„ÙƒÙ„ Ù…Ø±Ø¶ Ù„Ù„Ø¥Ø¨Ù‚Ø§Ø¡ Ø¹Ù„ÙŠÙ‡")
    ap.add_argument("--seed", type=int, default=42, help="Seed")
    ap.add_argument("--weight_step", type=float, default=0.25, help="Ø®Ø·ÙˆØ© Ø´Ø¨ÙƒØ© Ø§Ù„Ø£ÙˆØ²Ø§Ù† (0.25 ØªÙˆÙÙ‘Ø± Ø³Ø±Ø¹Ø© Ø¬ÙŠØ¯Ø©)")
    ap.add_argument("--verbose", action="store_true", help="Ø·Ø¨Ø§Ø¹Ø© ØªÙØ§ØµÙŠÙ„ Ø¥Ø¶Ø§ÙÙŠØ©")
    args = ap.parse_args()

    if not os.path.exists(args.csv):
        raise FileNotFoundError(f"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø§Ù„Ù…Ù„Ù: {args.csv}")

    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df = pd.read_csv(args.csv, encoding="utf-8", on_bad_lines="skip")
    df.columns = [c.strip() for c in df.columns]

    for col in (args.dept_col, args.disease_col):
        if col not in df.columns:
            raise ValueError(f"Ø¹Ù…ÙˆØ¯ '{col}' ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ CSV.")

    # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙŠØ²Ø§Øª (ØªØ«Ø¨ÙŠØª feature_cols Ù…Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ù„ÙŠØ³ØªØ®Ø¯Ù…Ù‡Ø§ ÙƒÙ„ Ù‚Ø³Ù…)
    X_all, feature_cols_fixed = build_features_like_training(
        df, feature_cols_training=None,
        target_cols=[args.dept_col, args.disease_col],
        ignore_cols=args.ignore
    )

    # Ø§Ù„Ø­Ø²Ù…Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    PKL = {
        "feature_cols": feature_cols_fixed,
        "group_defs": GROUPS,
        "derived_cols": DERIVED_COLS,
        "departments": {}
    }

    # ØªØ¯Ø±ÙŠØ¨ Ù„ÙƒÙ„ Ù‚Ø³Ù…
    departments = sorted(df[args.dept_col].astype(str).unique())
    print(f"ğŸ§­ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ù‚Ø³Ø§Ù…: {len(departments)} -> {departments}")

    for dept in departments:
        dfd = df[df[args.dept_col].astype(str) == str(dept)].copy()
        y_disease_str = dfd[args.disease_col].astype(str)

        # ØªØµÙÙŠØ© Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ù‚Ù„ÙŠÙ„Ø© Ø§Ù„Ø¹ÙŠÙ†Ø§Øª (ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±)
        counts = y_disease_str.value_counts()
        keep = counts[counts >= args.min_per_class].index
        dfd = dfd[y_disease_str.isin(keep)].copy()
        if dfd.shape[0] == 0:
            print(f"âš ï¸ Ø§Ù„Ù‚Ø³Ù… '{dept}': Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹ÙŠÙ†Ø§Øª ÙƒØ§ÙÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„ØªØµÙÙŠØ©. Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠÙ‡.")
            continue

        y_disease_str = dfd[args.disease_col].astype(str)
        # Ø§Ø³ØªØ®Ø¯Ù… Ù†ÙØ³ feature_cols_fixed Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§ØªØ³Ø§Ù‚
        Xd = dfd.drop(columns=[args.dept_col, args.disease_col] + args.ignore, errors="ignore")
        Xd = Xd.reindex(columns=[c for c in X_all.columns if c in Xd.columns], fill_value=0)
        # Ø£Ø¹Ø¯ Ø§Ù„Ø¨Ù†Ø§Ø¡ Ø¨Ù†ÙØ³ Ø§Ù„Ù…Ù†Ø·Ù‚ Ù„ÙƒÙŠ Ù„Ø§ Ù†ÙÙ‚Ø¯ Ø§Ù„Ù…Ø´ØªÙ‚Ø§Øª
        Xd, _ = build_features_like_training(
            dfd, feature_cols_training=feature_cols_fixed,
            target_cols=[args.dept_col, args.disease_col],
            ignore_cols=args.ignore
        )

        le = LabelEncoder()
        y = le.fit_transform(y_disease_str.values)
        classes = list(le.classes_)
        n_classes = len(classes)
        n_samples = len(y)

        print(f"\n===== Ù‚Ø³Ù…: {dept} | Ø¹ÙŠÙ†Ø§Øª: {n_samples} | Ø£Ù…Ø±Ø§Ø¶: {n_classes} =====")
        if n_classes < 2:
            # Ø­Ø§Ù„Ø© ÙØ¦Ø© ÙˆØ§Ø­Ø¯Ø©: Ù†Ø®Ø²Ù† Ù…Ø®Ù…Ù†Ù‹Ø§ Ø«Ø§Ø¨ØªÙ‹Ø§ (will always predict that disease)
            only = classes[0]
            print(f"âš ï¸ Ø§Ù„Ù‚Ø³Ù… '{dept}': Ù…Ø±Ø¶ ÙˆØ§Ø­Ø¯ ÙÙ‚Ø· Ø¨Ø¹Ø¯ Ø§Ù„ØªØµÙÙŠØ© ({only}). Ø³ÙŠØªÙ… ØªØ®Ø²ÙŠÙ† Ù…ÙØªÙ†Ø¨Ø¦ Ø«Ø§Ø¨Øª.")
            PKL["departments"][dept] = {
                "label_encoder": le,
                "feature_cols": feature_cols_fixed,
                "rf_cal": None, "gb_cal": None, "et_cal": None, "stack": None,
                "best_weights": {"rf":0.0,"gb":0.0,"et":0.0},
                "extra_estimators": [],
                "classes_": classes,
                "single_class": True,
                "single_class_label": only
            }
            continue

        # Ø§Ø®ØªÙŠØ§Ø± Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ø¨Ø­Ø³Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù‚Ø³Ù…
        calib = choose_calibration_method(n_samples)

        # Ø¨Ø­Ø« Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¹Ø¨Ø± ØªØ­Ù‚Ù‚ Ù…ØªÙ‚Ø§Ø·Ø¹
        splits = min(args.n_splits, np.min(np.bincount(y)))  # Ù„Ø§ ØªØªØ¬Ø§ÙˆØ² Ø£Ù‚Ù„ Ø­Ø¬Ù… ÙØ¦Ø©
        splits = max(3, min(splits, args.n_splits))          # Ù…Ù† 3 Ø¥Ù„Ù‰ n_splits
        best_w, cv_mean = cv_search_weights_for_dept(
            Xd, y, seed=args.seed, n_splits=splits, step=args.weight_step, verbose=args.verbose
        )

        # ØªØ¯Ø±ÙŠØ¨ ÙƒØ§Ù…Ù„ + Ù…Ø¹Ø§ÙŠØ±Ø©
        rf_cal, gb_cal, et_cal = train_calibrated_models_full(Xd, y, seed=args.seed, calib=calib)

        # Ø®Ø²Ù† Ø§Ù„Ù‚Ø³Ù…
        PKL["departments"][dept] = {
            "label_encoder": le,
            "feature_cols": feature_cols_fixed,
            "rf_cal": rf_cal, "gb_cal": gb_cal, "et_cal": et_cal, "stack": None,
            "best_weights": best_w,
            "extra_estimators": [],   # ÙŠÙ…ÙƒÙ† Ø¥Ø¶Ø§ÙØ© Ù†Ù…Ø§Ø°Ø¬ Ø¥Ø¶Ø§ÙÙŠØ© Ø¥Ù† Ø±ØºØ¨Øª Ù…Ø³ØªÙ‚Ø¨Ù„Ø§Ù‹
            "classes_": classes,
            "single_class": False,
            "cv_f1_macro": float(cv_mean),
            "calibration": calib
        }

        # Ø·Ø¨Ø§Ø¹Ø© Ù…ÙˆØ¬Ø² Ø³Ø±ÙŠØ¹ Ù„Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø³Ù… (ØªÙ‚Ø¯ÙŠØ± Ù…ØªÙØ§Ø¦Ù„)
        from sklearn.metrics import f1_score
        def _proba_in_order(mdl): return proba_in_label_encoder_order(mdl, Xd, n_classes, le)
        probas = {"rf": _proba_in_order(rf_cal), "gb": _proba_in_order(gb_cal), "et": _proba_in_order(et_cal)}
        P = blend_proba(probas, best_w, n_classes)
        yhat = np.argmax(P, axis=1)
        f1m_full = f1_score(y, yhat, average="macro")
        print(f"âœ… {dept}: Ø£ÙØ¶Ù„ Ø£ÙˆØ²Ø§Ù† {best_w} | F1_macro_CV={cv_mean:.3f} | F1_macro_full={f1m_full:.3f} | calib={calib}")

    # Ø­ÙØ¸ Ø§Ù„Ø­Ø²Ù…Ø©
    with open(args.out, "wb") as f:
        pickle.dump(PKL, f)
    print(f"\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø­Ø²Ù…Ø©: {args.out}")
    print("ğŸ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ù…Ø±Ø§Ø¶ Ù„ÙƒÙ„ Ù‚Ø³Ù….")

if __name__ == "__main__":
    main()
